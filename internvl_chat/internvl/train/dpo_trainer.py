import os
import torch

from torch.utils.data import Sampler

from transformers import Trainer
# from transformers.trainer import (
#     is_sagemaker_mp_enabled,
#     get_parameter_names,
#     has_length,
#     ALL_LAYERNORM_LAYERS,
#     ShardedDDPOption,
#     logger,
# )
from typing import List, Optional, Tuple
import torch
from torch import nn, Tensor
from torch.nn import Module
from torch.nn import functional as F


# def maybe_zero_3(param, ignore_status=False, name=None):
#     from deepspeed import zero
#     from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus
#     if hasattr(param, "ds_id"):
#         if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:
#             if not ignore_status:
#                 print(name, 'no ignore status')
#         with zero.GatheredParameters([param]):
#             param = param.data.detach().cpu().clone()
#     else:
#         param = param.detach().cpu().clone()
#     return param


# def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):
#     to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}
#     to_return = {k: maybe_zero_3(v, ignore_status=True, name=k).cpu() for k, v in to_return.items()}
#     return to_return


# def split_to_even_chunks(indices, lengths, num_chunks):
#     """
#     Split a list of indices into `chunks` chunks of roughly equal lengths.
#     """

#     if len(indices) % num_chunks != 0:
#         return [indices[i::num_chunks] for i in range(num_chunks)]

#     num_indices_per_chunk = len(indices) // num_chunks

#     chunks = [[] for _ in range(num_chunks)]
#     chunks_lengths = [0 for _ in range(num_chunks)]
#     for index in indices:
#         shortest_chunk = chunks_lengths.index(min(chunks_lengths))
#         chunks[shortest_chunk].append(index)
#         chunks_lengths[shortest_chunk] += lengths[index]
#         if len(chunks[shortest_chunk]) == num_indices_per_chunk:
#             chunks_lengths[shortest_chunk] = float("inf")

#     return chunks


# def get_modality_length_grouped_indices(lengths, batch_size, world_size, generator=None):
#     # We need to use torch for the random part as a distributed sampler will set the random seed for torch.
#     assert all(l != 0 for l in lengths), "Should not have zero length."
#     if all(l > 0 for l in lengths) or all(l < 0 for l in lengths):
#         # all samples are in the same modality
#         return get_length_grouped_indices(lengths, batch_size, world_size, generator=generator)
#     mm_indices, mm_lengths = zip(*[(i, l) for i, l in enumerate(lengths) if l > 0])
#     lang_indices, lang_lengths = zip(*[(i, -l) for i, l in enumerate(lengths) if l < 0])

#     mm_shuffle = [mm_indices[i] for i in get_length_grouped_indices(mm_lengths, batch_size, world_size, generator=None)]
#     lang_shuffle = [lang_indices[i] for i in get_length_grouped_indices(lang_lengths, batch_size, world_size, generator=None)]
#     megabatch_size = world_size * batch_size
#     mm_megabatches = [mm_shuffle[i : i + megabatch_size] for i in range(0, len(mm_shuffle), megabatch_size)]
#     lang_megabatches = [lang_shuffle[i : i + megabatch_size] for i in range(0, len(lang_shuffle), megabatch_size)]

#     last_mm = mm_megabatches[-1]
#     last_lang = lang_megabatches[-1]
#     additional_batch = last_mm + last_lang
#     megabatches = mm_megabatches[:-1] + lang_megabatches[:-1]
#     megabatch_indices = torch.randperm(len(megabatches), generator=generator)
#     megabatches = [megabatches[i] for i in megabatch_indices]

#     if len(additional_batch) > 0:
#         megabatches.append(sorted(additional_batch))

#     return [i for megabatch in megabatches for i in megabatch]


# def get_length_grouped_indices(lengths, batch_size, world_size, generator=None, merge=True):
#     # We need to use torch for the random part as a distributed sampler will set the random seed for torch.
#     indices = torch.randperm(len(lengths), generator=generator)
#     megabatch_size = world_size * batch_size
#     megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]
#     megabatches = [sorted(megabatch, key=lambda i: lengths[i], reverse=True) for megabatch in megabatches]
#     megabatches = [split_to_even_chunks(megabatch, lengths, world_size) for megabatch in megabatches]

#     return [i for megabatch in megabatches for batch in megabatch for i in batch]


# class LengthGroupedSampler(Sampler):
#     r"""
#     Sampler that samples indices in a way that groups together features of the dataset of roughly the same length while
#     keeping a bit of randomness.
#     """

#     def __init__(
#         self,
#         batch_size: int,
#         world_size: int,
#         lengths: Optional[List[int]] = None,
#         generator=None,
#         group_by_modality: bool = False,
#     ):
#         if lengths is None:
#             raise ValueError("Lengths must be provided.")

#         self.batch_size = batch_size
#         self.world_size = world_size
#         self.lengths = lengths
#         self.generator = generator
#         self.group_by_modality = group_by_modality

#     def __len__(self):
#         return len(self.lengths)

#     def __iter__(self):
#         if self.group_by_modality:
#             indices = get_modality_length_grouped_indices(self.lengths, self.batch_size, self.world_size, generator=self.generator)
#         else:
#             indices = get_length_grouped_indices(self.lengths, self.batch_size, self.world_size, generator=self.generator)
#         return iter(indices)


# class BaseTrainer(Trainer):

#     def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
#         if self.train_dataset is None or not has_length(self.train_dataset):
#             return None

#         if self.args.group_by_modality_length:
#             lengths = self.train_dataset.modality_lengths
#             return LengthGroupedSampler(
#                 self.args.train_batch_size,
#                 world_size=self.args.world_size * self.args.gradient_accumulation_steps,
#                 lengths=lengths,
#                 group_by_modality=True,
#             )
#         else:
#             return super()._get_train_sampler()

#     def create_optimizer(self):
#         """
#         Setup the optimizer.

#         We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
#         Trainer's init through `optimizers`, or subclass and override this method in a subclass.
#         """
#         if is_sagemaker_mp_enabled():
#             return super().create_optimizer()
#         if self.sharded_ddp == ShardedDDPOption.SIMPLE:
#             return super().create_optimizer()

#         opt_model = self.model

#         if self.optimizer is None:
#             decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)
#             decay_parameters = [name for name in decay_parameters if "bias" not in name]
#             if self.args.mm_projector_lr is not None:
#                 projector_parameters = [name for name, _ in opt_model.named_parameters() if "mm_projector" in name]
#                 optimizer_grouped_parameters = [
#                     {
#                         "params": [
#                             p for n, p in opt_model.named_parameters() if (n in decay_parameters and n not in projector_parameters and p.requires_grad)
#                         ],
#                         "weight_decay": self.args.weight_decay,
#                     },
#                     {
#                         "params": [
#                             p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n not in projector_parameters and p.requires_grad)
#                         ],
#                         "weight_decay": 0.0,
#                     },
#                     {
#                         "params": [
#                             p for n, p in opt_model.named_parameters() if (n in decay_parameters and n in projector_parameters and p.requires_grad)
#                         ],
#                         "weight_decay": self.args.weight_decay,
#                         "lr": self.args.mm_projector_lr,
#                     },
#                     {
#                         "params": [
#                             p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)
#                         ],
#                         "weight_decay": 0.0,
#                         "lr": self.args.mm_projector_lr,
#                     },
#                 ]
#             else:
#                 optimizer_grouped_parameters = [
#                     {
#                         "params": [
#                             p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)
#                         ],
#                         "weight_decay": self.args.weight_decay,
#                     },
#                     {
#                         "params": [
#                             p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)
#                         ],
#                         "weight_decay": 0.0,
#                     },
#                 ]

#             optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)

#             if self.sharded_ddp == ShardedDDPOption.SIMPLE:
#                 self.optimizer = OSS(
#                     params=optimizer_grouped_parameters,
#                     optim=optimizer_cls,
#                     **optimizer_kwargs,
#                 )
#             else:
#                 self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
#                 if optimizer_cls.__name__ == "Adam8bit":
#                     import bitsandbytes

#                     manager = bitsandbytes.optim.GlobalOptimManager.get_instance()

#                     skipped = 0
#                     for module in opt_model.modules():
#                         if isinstance(module, nn.Embedding):
#                             skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())
#                             logger.info(f"skipped {module}: {skipped/2**20}M params")
#                             manager.register_module_override(module, "weight", {"optim_bits": 32})
#                             logger.debug(f"bitsandbytes: will optimize {module} in fp32")
#                     logger.info(f"skipped: {skipped/2**20}M params")

#         return self.optimizer

#     def _save_checkpoint(self, model, trial, metrics=None):
#         if getattr(self.args, 'tune_mm_mlp_adapter', False):
#             from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
#             checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}"

#             run_dir = self._get_output_dir(trial=trial)
#             output_dir = os.path.join(run_dir, checkpoint_folder)

#             # Only save Adapter
#             keys_to_match = ['mm_projector', 'vision_resampler']
#             if getattr(self.args, "use_im_start_end", False):
#                 keys_to_match.extend(['embed_tokens', 'embed_in'])

#             weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)

#             if self.args.local_rank == 0 or self.args.local_rank == -1:
#                 self.model.config.save_pretrained(output_dir)
#                 torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))
#         else:
#             super(LLaVATrainer, self)._save_checkpoint(model, trial, metrics)

#     def _save(self, output_dir: Optional[str] = None, state_dict=None):
#         if getattr(self.args, 'tune_mm_mlp_adapter', False):
#             pass
#         else:
#             super(LLaVATrainer, self)._save(output_dir, state_dict)


def get_batch_logps(logits: torch.FloatTensor, labels: torch.LongTensor, return_per_token_logp=False, return_all=False) -> torch.FloatTensor:
    """Compute the log probabilities of the given labels under the given logits.

    Args:
        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)
        labels: Labels for which to compute the log probabilities. Label tokens with a value of -100 are ignored. Shape: (batch_size, sequence_length)
    Returns:
        A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.
    """
    assert logits.shape[:-1] == labels.shape

    labels = labels[:, 1:].clone()
    logits = logits[:, :-1, :]
    loss_mask = (labels != -100)

    # dummy token; we'll ignore the losses on these tokens later
    labels[labels == -100] = 0

    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2,
                                   index=labels.unsqueeze(2)).squeeze(2)

    log_prob = (per_token_logps * loss_mask).sum(-1)
    average_log_prob = log_prob / loss_mask.sum(-1)

    # print(per_token_logps.shape, labels.shape)
    if return_per_token_logp:
        return per_token_logps

    if return_all:
        return per_token_logps, log_prob, average_log_prob

    return log_prob, average_log_prob


def compute_weighted_logp(per_token_logp, labels, token_weight, use_average):
    loss_mask = (labels[:, 1:].clone() != -100)
    # print(f'compute wlogp {labels.shape} {loss_mask.shape}, {token_weight.shape}, {per_token_logp.shape}')
    weighted_mask = token_weight * loss_mask
    logp = (per_token_logp * weighted_mask).sum(-1)

    average_logp = logp / weighted_mask.sum(-1)
    if use_average:
        return average_logp
    return logp


def forward_DPO(model, input_ids, labels, attention_mask, pixel_values, image_flags, **kwargs):
    token_weighted = kwargs.pop('token_weighted', False)
    dpo_use_average = kwargs.pop('dpo_use_average', False)

    output = model(
        input_ids=input_ids,
        labels=labels,
        attention_mask=attention_mask,
        pixel_values=pixel_values,
        image_flags=image_flags,
        **kwargs
    )
    # print(f'Shape of output.logits', output.logits.shape)
    # print(f'Shape of labels', labels.shape)
    # output.logits = output.logits[:, 575:, :]
    if token_weighted:
        token_log_prob = get_batch_logps(output.logits, labels, return_per_token_logp=True)
        return token_log_prob
    else:
        log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_per_token_logp=False)
        if dpo_use_average:
            return average_log_prob
        return log_prob


def dpo_loss(policy_chosen_logps: torch.FloatTensor,
             policy_rejected_logps: torch.FloatTensor,
             reference_chosen_logps: torch.FloatTensor,
             reference_rejected_logps: torch.FloatTensor,
             beta: float,
             reference_free: bool = False) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
    """Compute the DPO loss for a batch of policy and reference model log probabilities.

    Args:
        policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
        policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)
        reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)
        reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)
        beta: Temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.
        reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.

    Returns:
        A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).
        The losses tensor contains the DPO loss for each example in the batch.
        The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.
    """
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps

    if reference_free:
        ref_logratios = 0

    logits = pi_logratios - ref_logratios

    losses = -F.logsigmoid(beta * logits)
    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()

    return losses, chosen_rewards, rejected_rewards


class DPOTrainer(Trainer):

    def compute_loss(self, model: Module, inputs: dict, return_outputs=False):

        data_dict = inputs
        win_input_ids = data_dict.pop('win_input_ids')
        rej_input_ids = data_dict.pop('rej_input_ids')

        win_labels = data_dict.pop('win_labels')
        rej_labels = data_dict.pop('rej_labels')

        win_attention_mask = data_dict.pop('win_attention_mask')
        rej_attention_mask = data_dict.pop('rej_attention_mask')

        # ref_win_avg_logp = data_dict.pop('ref_win_avg_logp')
        # ref_rej_avg_logp = data_dict.pop('ref_rej_avg_logp')
        # ref_win_logp = data_dict.pop('ref_win_logp')
        # ref_rej_logp = data_dict.pop('ref_rej_logp')
        # ref_win_per_token_logp = data_dict.pop('ref_win_per_token_logp')
        # ref_rej_per_token_logp = data_dict.pop('ref_rej_per_token_logp')
        # if self.args.dpo_use_average:
        #     ref_win_logp = ref_win_avg_logp
        #     ref_rej_logp = ref_rej_avg_logp

        beta = data_dict.pop('beta')

        concatenated_input_ids = data_dict.pop('concatenated_input_ids')
        concatenated_labels = data_dict.pop('concatenated_labels')
        concatenated_attention_mask = data_dict.pop('concatenated_attention_mask')
        concatenated_pixel_values=data_dict.pop('concatenated_pixel_values'),
        concatenated_image_flags=data_dict.pop('concatenated_image_flags'),

        # TODO: we do not have this information
        # win_token_weight = data_dict.pop('win_token_weight')
        # rej_token_weight = data_dict.pop('rej_token_weight')
        # concatenated_token_weight = data_dict.pop('concatenated_token_weight')

        concatenated_logp = forward_DPO(model,
                                        concatenated_input_ids,
                                        concatenated_labels,
                                        concatenated_attention_mask,
                                        concatenated_pixel_values,
                                        concatenated_image_flags,
                                        token_weighted=self.args.dpo_token_weighted,
                                        dpo_use_average=self.args.dpo_use_average,
                                        **data_dict)
        win_size = win_input_ids.shape[0]
        rej_size = rej_input_ids.shape[0]
        assert win_size == rej_size

        # if self.args.dpo_token_weighted:
        #     ref_win_logp = compute_weighted_logp(ref_win_per_token_logp, win_labels, win_token_weight, self.args.dpo_use_average)
        #     ref_rej_logp = compute_weighted_logp(ref_rej_per_token_logp, rej_labels, rej_token_weight, self.args.dpo_use_average)
        #     concatenated_logp = compute_weighted_logp(concatenated_logp, concatenated_labels,concatenated_token_weight, self.args.dpo_use_average)

        #     if torch.any(torch.isnan(ref_win_logp)):
        #         print(f'ref_win_logp fail', flush=True)
        #         exit()
        #     if torch.any(torch.isnan(ref_rej_logp)):
        #         print(f'ref_rej_logp fail', flush=True)
        #         exit()
        #     if torch.any(torch.isnan(concatenated_logp)):
        #         print(f'concatenated_logp fail', flush=True)
        #         exit()

        policy_win_logp, policy_rej_logp = concatenated_logp.split([win_size, rej_size])

        if self.args.past_index >= 0:
            raise NotImplementedError

        # TODO: No ref model is used.
        losses, chosen_rewards, rejected_rewards = dpo_loss(policy_win_logp,
                                                            policy_rej_logp,
                                                            policy_win_logp,
                                                            policy_rej_logp,
                                                            beta=beta,
                                                            reference_free=True)

        reward_accuracies = (chosen_rewards > rejected_rewards).float()
        # loss = losses.mean()

        # do SFT
        # loss = - policy_win_logp.mean()
        SFT_weight = float(os.environ.get('SFT_weight', 0.5))
        DPO_weight = float(os.environ.get('DPO_weight', 1.0))
        print(f"+++ SFT loss: {- SFT_weight * policy_win_logp.mean()}")
        loss = DPO_weight * losses.mean() - SFT_weight * policy_win_logp.mean()
        # loss = DPO_weight * losses.mean() - SFT_weight * policy_rej_logp.mean()

        train_test = 'train' if model.training else 'test'
        metrics = {}
        metrics[f'rewards_{train_test}/chosen'] = self._nested_gather(chosen_rewards.mean()).mean().item()
        metrics[f'rewards_{train_test}/rejected'] = self._nested_gather(rejected_rewards.mean()).mean().item()
        metrics[f'rewards_{train_test}/accuracies'] = self._nested_gather(reward_accuracies.mean()).mean().item()
        metrics[f'rewards_{train_test}/margins'] = metrics[f'rewards_{train_test}/chosen'] - metrics[f'rewards_{train_test}/rejected']
        metrics[f'logps_{train_test}/rejected'] = self._nested_gather(policy_rej_logp.mean()).mean().item()
        metrics[f'logps_{train_test}/chosen'] = self._nested_gather(policy_win_logp.mean()).mean().item()
        # metrics[f'logps_{train_test}/ref_rejected'] = self._nested_gather(ref_rej_logp.mean()).mean().item()
        # metrics[f'logps_{train_test}/ref_chosen'] = self._nested_gather(ref_win_logp.mean()).mean().item()
        # metrics[f'batch_size'] = len(win_labels)
        self.log(metrics)

        return loss